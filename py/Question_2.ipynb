{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087e0362",
   "metadata": {},
   "source": [
    "# Using a Neural Network approach to see whether an author's name constitutes a critical/commercial hit or flop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaaa0c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import functions\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df3602",
   "metadata": {},
   "source": [
    "Checking the amount of unique authors in the dataset. Credit to ChatGPT for showing how to do so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ecad8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12877\n"
     ]
    }
   ],
   "source": [
    "df_metadata = functions.get_data()\n",
    "num_authors = df_metadata['author_name'].nunique()\n",
    "print(num_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ff26f",
   "metadata": {},
   "source": [
    "Over 12,000 authors. Statiscally, most authors have 1 book, so I will check if that is the case. Credit to ChatGPT which showed me how to code up a solution to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ff4977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12877.000000\n",
       "mean         1.553157\n",
       "std          1.800488\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max         76.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata['author_name'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b528253",
   "metadata": {},
   "source": [
    "This shows that the majority of authors have only 1 book to their names making it likely that they are debut authors. Lastly, it can be shown that the maximum number of books an author has got on the Amazon storefront is 76. This shows that author name alone is not enough to determine a critical or commercial success. As such, my neural network will use the context of this information to determine whether the general audience would support a given book both financially and critically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b652e4",
   "metadata": {},
   "source": [
    "## The Foundations of the Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2779f27",
   "metadata": {},
   "source": [
    "Credit to ChatGPT for showing me how to lay the groundwork for my neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "844c99bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['rating number'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 3️⃣ Get your training/test split\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m X_author_train, X_author_test, X_num_train, X_num_test, y_train, y_test = \u001b[43mfunctions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_train_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m num_authors = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(X_author_train))  \u001b[38;5;66;03m# ~12,000 in your dataset\u001b[39;00m\n\u001b[32m     44\u001b[39m num_numeric_features = X_num_train.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ACT-CW2---UP2107357/py/functions.py:47\u001b[39m, in \u001b[36mtest_train_split\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Accounting for the other numeric features within the dataset:\u001b[39;00m\n\u001b[32m     46\u001b[39m numeric_metrics = [\u001b[33m'\u001b[39m\u001b[33mauthor_book_count\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdebut\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrating number\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33maverage_rating\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m x_num = \u001b[43mdf_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_metrics\u001b[49m\u001b[43m]\u001b[49m.fillna(\u001b[32m0\u001b[39m).values\n\u001b[32m     48\u001b[39m scaler = StandardScaler()\n\u001b[32m     49\u001b[39m x_num = scaler.fit_transform(x_num)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['rating number'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "# 1️⃣ Define a custom dataset\n",
    "class BooksDataset(Dataset):\n",
    "    def __init__(self, authors, features, labels):\n",
    "        self.authors = torch.tensor(authors, dtype=torch.long)       # author IDs for embeddings\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)  # numeric features\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)      # target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.authors[idx], self.features[idx], self.labels[idx]\n",
    "\n",
    "# 2️⃣ Neural network combining embeddings + numeric features\n",
    "class AuthorNet(nn.Module):\n",
    "    def __init__(self, num_authors, embedding_dim, num_numeric_features):\n",
    "        super(AuthorNet, self).__init__()\n",
    "        # Author embedding\n",
    "        self.embedding = nn.Embedding(num_authors, embedding_dim)\n",
    "        # Fully connected layers for numeric features\n",
    "        self.fc_numeric = nn.Sequential(\n",
    "            nn.Linear(num_numeric_features, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Combine embeddings + numeric features\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + 16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # binary output\n",
    "        )\n",
    "\n",
    "    def forward(self, author_ids, numeric_features):\n",
    "        x_author = self.embedding(author_ids)\n",
    "        x_numeric = self.fc_numeric(numeric_features)\n",
    "        x = torch.cat([x_author, x_numeric], dim=1)\n",
    "        x = self.fc_combined(x)\n",
    "        return x\n",
    "\n",
    "# 3️⃣ Get your training/test split\n",
    "X_author_train, X_author_test, X_num_train, X_num_test, y_train, y_test = functions.test_train_split()\n",
    "\n",
    "num_authors = len(set(X_author_train))  # ~12,000 in your dataset\n",
    "num_numeric_features = X_num_train.shape[1]\n",
    "embedding_dim = 16  # you can adjust\n",
    "\n",
    "# 4️⃣ Create datasets and dataloaders\n",
    "train_dataset = BooksDataset(X_author_train, X_num_train, y_train)\n",
    "test_dataset = BooksDataset(X_author_test, X_num_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 5️⃣ Instantiate model, loss, optimizer\n",
    "model = AuthorNet(num_authors=num_authors, embedding_dim=embedding_dim, num_numeric_features=num_numeric_features)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 6️⃣ Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for author_ids, numeric_features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(author_ids, numeric_features).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 7️⃣ Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for author_ids, numeric_features, labels in test_loader:\n",
    "        outputs = model(author_ids, numeric_features).squeeze()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test accuracy: {correct/total:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f5c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author_name', 'publisher', 'publisher_date', 'format', 'page_count',\n",
       "       'language', 'category_level_2_sub', 'category_level_3_detail',\n",
       "       'average_rating', 'rating_number', 'price_numeric', 'maybe_date',\n",
       "       'author_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
