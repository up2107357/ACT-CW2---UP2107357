{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b6d07a",
   "metadata": {},
   "source": [
    "# Question 3: How does pretraining (and epochs and batches) affect the performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c87ac",
   "metadata": {},
   "source": [
    "The plan intended to answer this question is simple: the neural network from Question 2 will be reused with changes to the number of epochs and the batch size to see whether the accuracy increases or decreases. I will start by reusing all the dependencies for Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462376b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import functions\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "%pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa74d3",
   "metadata": {},
   "source": [
    "Next, the same neural network will be used from Question 2. Credit to ChatGPT for showing me how to create it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84793d",
   "metadata": {},
   "source": [
    "I will double the batch size and number of epochs to see if there is any change in the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e1637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6070\n",
      "Epoch 2, Loss: 0.4525\n",
      "Epoch 3, Loss: 0.2939\n",
      "Epoch 4, Loss: 0.1709\n",
      "Epoch 5, Loss: 0.1075\n",
      "Epoch 6, Loss: 0.0762\n",
      "Epoch 7, Loss: 0.0592\n",
      "Epoch 8, Loss: 0.0489\n",
      "Epoch 9, Loss: 0.0423\n",
      "Epoch 10, Loss: 0.0364\n",
      "Epoch 11, Loss: 0.0320\n",
      "Epoch 12, Loss: 0.0287\n",
      "Epoch 13, Loss: 0.0259\n",
      "Epoch 14, Loss: 0.0236\n",
      "Epoch 15, Loss: 0.0216\n",
      "Epoch 16, Loss: 0.0194\n",
      "Epoch 17, Loss: 0.0176\n",
      "Epoch 18, Loss: 0.0160\n",
      "Epoch 19, Loss: 0.0146\n",
      "Epoch 20, Loss: 0.0134\n",
      "Test Accuracy: 0.985\n",
      "Test F1 Score: 0.976\n"
     ]
    }
   ],
   "source": [
    "class BooksDataset(Dataset):\n",
    "    def __init__(self, authors, features, labels):\n",
    "        self.authors = torch.tensor(authors, dtype=torch.long)       # author IDs for embeddings\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)  # numeric features\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)      # target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.authors[idx], self.features[idx], self.labels[idx]\n",
    "    \n",
    "class AuthorNet(nn.Module):\n",
    "    def __init__(self, num_authors, embedding_dim, num_numeric_features):\n",
    "        super(AuthorNet, self).__init__()\n",
    "        # Author embedding\n",
    "        self.embedding = nn.Embedding(num_authors, embedding_dim)\n",
    "        # Fully connected layers for numeric features\n",
    "        self.fc_numeric = nn.Sequential(\n",
    "            nn.Linear(num_numeric_features, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Combine embeddings + numeric features\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + 16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # binary output\n",
    "        )\n",
    "\n",
    "    def forward(self, author_ids, numeric_features):\n",
    "        x_author = self.embedding(author_ids)\n",
    "        x_numeric = self.fc_numeric(numeric_features)\n",
    "        x = torch.cat([x_author, x_numeric], dim=1)\n",
    "        x = self.fc_combined(x)\n",
    "        return x\n",
    "\n",
    "X_author_train, X_author_test, X_num_train, X_num_test, y_train, y_test = functions.test_train_split()\n",
    "\n",
    "# Map author IDs to consecutive integers as there is nearly 13,000 separate authors:\n",
    "\n",
    "author_to_idx = {author: i for i, author in enumerate(sorted(set(X_author_train)))}\n",
    "X_author_train = np.array([author_to_idx[a] for a in X_author_train])\n",
    "X_author_test = np.array([author_to_idx.get(a, 0) for a in X_author_test])  # unknown authors -> 0\n",
    "num_authors = len(author_to_idx)\n",
    "\n",
    "# Scaling the numeric features as most of them are over 10,000 and they don't scale linearly:\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num_train = scaler.fit_transform(X_num_train)\n",
    "X_num_test = scaler.transform(X_num_test)\n",
    "\n",
    "num_numeric_features = X_num_train.shape[1]\n",
    "embedding_dim = 16 \n",
    "\n",
    "# Defining the two datasets:\n",
    "\n",
    "train_dataset = BooksDataset(X_author_train, X_num_train, y_train)\n",
    "test_dataset = BooksDataset(X_author_test, X_num_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AuthorNet(num_authors=num_authors, embedding_dim=embedding_dim, num_numeric_features=num_numeric_features)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for author_ids, numeric_features, labels in train_loader:\n",
    "        author_ids = author_ids.to(device)\n",
    "        numeric_features = numeric_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(author_ids, numeric_features).view(-1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad(): # no.grad() disables gradient calculation as Tensor.backward() will not be called. This reduces memory consumption. Credit: Pytorch Reference API\n",
    "    for author_ids, numeric_features, labels in test_loader:\n",
    "        author_ids = author_ids.to(device)\n",
    "        numeric_features = numeric_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(author_ids, numeric_features).view(-1)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Test F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684260e8",
   "metadata": {},
   "source": [
    "The accuracy is lower than expected at an impressive 98.5%, just 0.2% below the initial batch size of 64 and epochs of 10. The F1 score is also 0.3% below what was previously found in Question 2. These discrepencies can be placed down to a possible lapse in training data where some of the data may have been miscalculated or the neural network was expecting niche markets to support their niche products. Seeing as the majority of the books in the dataset were highly rated, the accruacy and F1 score would be skewed to a positive light regardless. \n",
    "\n",
    "This could have been improved by finding a more balanced dataset or finding the average review from a set number. The only problem presented with that latter idea, is that most books in the dataset had less than 100 reviews total; hence, the data would still be skewed. Thus, more reviews would remove this statistical error within the findings of the neural networks. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
